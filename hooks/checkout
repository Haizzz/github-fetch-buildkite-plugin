#!/usr/bin/env bash

PATCH_VAR=$(cat <<EOF
diff --git a/production/buildkite/pipelines/canva-web-build/pipeline.ts b/production/buildkite/pipelines/canva-web-build/pipeline.ts
index 6a948267e6fd..61827714ec67 100644
--- a/production/buildkite/pipelines/canva-web-build/pipeline.ts
+++ b/production/buildkite/pipelines/canva-web-build/pipeline.ts
@@ -269,52 +269,52 @@ export async function getPipeline(args: PipelineArgs): Promise<Pipeline> {
       generateCanvaBuildTrigger.bind(null, env));
 
   const greenBlockingSteps = [
-    await checkEngines(args, env),
-    await checkBazelDependencies(args),
-    await checkBazelFiles(args),
-    await buildAllBazel(args, env),
-    ...packageGroupPipelineTriggerSteps,
-    await checkAppOpenApiCodegen(args),
-    await checkProtogen(args, env),
-    await checkProtofication(args),
-    await checkOwners(args),
+    // await checkEngines(args, env),
+    // await checkBazelDependencies(args),
+    // await checkBazelFiles(args),
+    // await buildAllBazel(args, env),
+    // ...packageGroupPipelineTriggerSteps,
+    // await checkAppOpenApiCodegen(args),
+    // await checkProtogen(args, env),
+    // await checkProtofication(args),
+    // await checkOwners(args),
     await checkLfs(args),
-    await checkFlags(args, env),
-    await checkJooq(args),
-    await checkLockedFiles(args),
-    await checkMigrations(args),
-    await shellcheck(args, shellFileFinder),
-    await blackFormatter(args, pythonFileFinder, blackImage),
-    await eslint(args),
-    await compileJsCfe(args),
-    await compileJsWebclient(args),
-    await testJsApi(args),
-    await testJsCfeModule(args),
-    await testJsCfeScript(args),
-    await testJsWebclient(args),
-    await testPipelineGenerators(args),
-    await checkTests(args, env),
-    await checkProperties(args, env),
-    backend,
-    frontend,
-    ...annotatedTestSteps,
-    await testSelenium({ ...rest, ghControlUserEnv }, [backend, frontend], env),
-    ...await getE2ETriggerSteps(args, env, ghControlUserEnv, [backend, frontend]),
-    ...await getIOSWebXTriggerSteps(args, env, ghControlUserEnv, [backend, frontend]),
+    // await checkFlags(args, env),
+    // await checkJooq(args),
+    // await checkLockedFiles(args),
+    // await checkMigrations(args),
+    // await shellcheck(args, shellFileFinder),
+    // await blackFormatter(args, pythonFileFinder, blackImage),
+    // await eslint(args),
+    // await compileJsCfe(args),
+    // await compileJsWebclient(args),
+    // await testJsApi(args),
+    // await testJsCfeModule(args),
+    // await testJsCfeScript(args),
+    // await testJsWebclient(args),
+    // await testPipelineGenerators(args),
+    // await checkTests(args, env),
+    // await checkProperties(args, env),
+    // backend,
+    // frontend,
+    // ...annotatedTestSteps,
+    // await testSelenium({ ...rest, ghControlUserEnv }, [backend, frontend], env),
+    // ...await getE2ETriggerSteps(args, env, ghControlUserEnv, [backend, frontend]),
+    // ...await getIOSWebXTriggerSteps(args, env, ghControlUserEnv, [backend, frontend]),
   ];
 
 
-  pipeline.add(await annotateFailures(
-      args,
-      [{
-        // TODO(Uri): change this so it can figure out which step the results come from
-        label: 'Pipeline',
-        artifact_path: "**/*_test_results/*-test-*.xml",
-        type: 'junit',
-      }],
-      annotatedTestSteps,
-      env,
-  ));
+  // pipeline.add(await annotateFailures(
+  //     args,
+  //     [{
+  //       // TODO(Uri): change this so it can figure out which step the results come from
+  //       label: 'Pipeline',
+  //       artifact_path: "**/*_test_results/*-test-*.xml",
+  //       type: 'junit',
+  //     }],
+  //     annotatedTestSteps,
+  //     env,
+  // ));
   if (current !== experimental) {
     pipeline.add(await updateCheckpoint(
         { ...rest, env },
@@ -329,7 +329,7 @@ export async function getPipeline(args: PipelineArgs): Promise<Pipeline> {
         experimental,
     ));
   }
-  pipeline.add(...greenBlockingSteps);
-  pipeline.add(await updateCheckpoint({ ...rest, env }, 'green', greenBlockingSteps, current));
+  // pipeline.add(...greenBlockingSteps);
+  // pipeline.add(await updateCheckpoint({ ...rest, env }, 'green', greenBlockingSteps, current));
   return pipeline;
 }
 
 
diff --git a/production/test/check_lfs.sh b/production/test/check_lfs.sh
index 3a3b67b030a9..e271b6bcf85a 100755
--- a/production/test/check_lfs.sh
+++ b/production/test/check_lfs.sh
@@ -27,18 +27,31 @@ exit_handler() {
 
     temp="\$(jq --arg code "\${last_error_code}" '.exitcode=\$code' <<< "\${temp}")"
     cat <<< "\${temp}" > "\${log_file_name}"
-  fi
 
-  aws lambda invoke \\ 
+    aws lambda invoke \\ 
+        --function-name write-infra-logs \\ 
+        --payload "\$(cat "\${log_file_name}")" \\ 
+        --region us-east-1 \\ 
+        "\${WORK_DIR}/blk_upload_log.json" \\ 
+    || \\ 
+    aws lambda invoke \\ 
       --function-name write-infra-logs \\ 
-      --payload "\$(cat "\${log_file_name}")" \\ 
+      --payload "{ \\"event_type\\": \\"check-lfs\\", \\"result\\": \\"upload_fail\\" }" \\ 
       --region us-east-1 \\ 
-      "\${WORK_DIR}/blk_upload_log.json" || true 
+      "\${WORK_DIR}/blk_upload_log.json" \\ 
+    || \\ 
+    true 
+  fi 
 
   buildkite-agent artifact upload "\${log_file_name}" || true
   buildkite-agent artifact upload "\${WORK_DIR}/blk_upload_log.json" || true
 
   rm -rf "\${WORK_DIR}"
+
+  if [[ "\${last_error_code}" -ne 0 ]]; then
+    exit 1
+  fi
+
   exit 0
 }
 
@@ -167,8 +180,7 @@ main() {
             # compare with content in file system
             git lfs pointer --stdin --file="\${changed_file}" > /dev/null 2>&1 || \\ 
             (log_error "File \${changed_file} LFS integrity is broken."; append_check_error "\${changed_file}"; mark_failed) || \\ 
-            true
-
+              log_info "LFS integrity check completed with failure."; false 
       else
         log_info "Skip LFS check for file \${changed_file}"
         append_timestamp_log "\${changed_file}" "normal"


diff --git a/.buildkite/hooks/post-checkout b/.buildkite/hooks/post-checkout
index f93956e11e91..08df4d38f020 100755
--- a/.buildkite/hooks/post-checkout
+++ b/.buildkite/hooks/post-checkout
@@ -18,7 +18,7 @@ assert_clean_checkout() {
 }
 
 main() {
-  assert_clean_checkout
+  true
 }
 
 main "$@"

 diff --git a/production/buildkite/_buildkite_pipeline.sh b/production/buildkite/_buildkite_pipeline.sh
index ec2e4b40df29..54abbf7ed9de 100755
--- a/production/buildkite/_buildkite_pipeline.sh
+++ b/production/buildkite/_buildkite_pipeline.sh
@@ -15,7 +15,7 @@ source "\${REPO_DIR}/tools/bash/_logging.sh"
 # shellcheck source=tools/buildkite/plugins/docker-ecr-auth/_auth.sh
 source "\${REPO_DIR}/tools/buildkite/plugins/docker-ecr-auth/_auth.sh"
 
-BUILDKITE_GITHUB_CHECKOUT_PLUGIN_VERSION="\${GITHUB_CHECKOUT_PLUGIN_VERSION:-0ae86092625c2e375232cf8a1c7b01f0096059a9}"
+BUILDKITE_GITHUB_CHECKOUT_PLUGIN_VERSION="\${GITHUB_CHECKOUT_PLUGIN_VERSION:-a88d54105ed5f8665287a01c816389598e5e9a77}"
 # shellcheck disable=SC2034
 BUILDKITE_GITHUB_CHECKOUT_PLUGIN="arromer/github-fetch#\${BUILDKITE_GITHUB_CHECKOUT_PLUGIN_VERSION}"
 # shellcheck disable=SC2034

EOF
)

set -euo pipefail

if [[ -n "${BUILDKITE_PLUGIN_GITHUB_FETCH_BASH_PREFIX:-}" ]]; then
  eval "${BUILDKITE_PLUGIN_GITHUB_FETCH_BASH_PREFIX}"
fi

# The maximum amount of time (in seconds) that a remote Git operation (fetch, pull, push) can take.
# If not specified by the user, no timeout will be applied to Git remote operations.
GIT_REMOTE_TIMEOUT="${BUILDKITE_PLUGIN_GITHUB_FETCH_GIT_REMOTE_TIMEOUT:-0}"

# Here is the exit code to be returned by this script when a remote Git operations times out.
# Having a specific exit code for these scenarions allows to configure Buildkite pipelines to retry
# the whole step without hiding underlying issues.
# See: https://buildkite.com/docs/pipelines/command-step#automatic-retry-attributes
#
# If not specified by the user, it defaults to 110 which is the standard exit code for connection timeout.
GIT_REMOTE_TIMEOUT_EXIT_CODE="${BUILDKITE_PLUGIN_GITHUB_FETCH_GIT_REMOTE_TIMEOUT_EXIT_CODE:-110}"

# Set to `true` forces a fresh clone from the remote to initialize the local copy for the first time
# on the agent.
#
# Otherwise it will re-use a cached copy from S3 for the first time or re-use the local repository.
BUILDKITE_CLEAN_CHECKOUT="${BUILDKITE_CLEAN_CHECKOUT:-false}"

# Prints an info line to stdout.
log_info() {
  echo "$(date '+[%Y-%m-%d %H:%M:%S]') INFO: $*"
}

# Checks if an env var is set
# Arguments:
# $1: var name
check_set() {
  local name="$1"
  if [[ -z "${!name:-}" ]]; then
    echo "ERROR: ${name} not set"
    exit 1
  fi

  echo "check_set: ${name}=${!name}"
}

# Checks for failed exit codes returned by a timeout command.
# If the underlying command fails due to timeout, this function stops the current execution returning either the
# TIMEOUT exit code (124) or the override value provided by the caller.
# In any other non-timeout failure scenarios, the current execution is stopped but the underlying command's exit
# code is returned unmodified.
# Arguments:
#   $1: The exit code returned by the timeout command.
#   $2: The exit code to be returned when the underlying command times out.
check_timeout_exit_code() {
  local exit_code="$1"
  local timeout_exit_code_override="${2:-}"
  if [[ "${exit_code}" -eq 124 && -n "${timeout_exit_code_override}" ]]; then
    return "${timeout_exit_code_override}"
  elif [[ "${exit_code}" -ne 0 ]]; then
    return "${exit_code}"
  fi
}

copy_checkout_from_s3() {
  local s3_url="$1"
  local checkout

  log_info "Getting checkout from S3"

  clean_checkout_dir

  # Find the most recent checkout in S3.
  checkout=$(aws s3 ls "${s3_url}/" \
      | (sort -r -k 4 || true) \
      | head -n1 \
      | awk '{print $4}'
  )

  pushd .. >/dev/null
  # shellcheck disable=SC2064
  trap "rm ${PWD}/${checkout}" EXIT
  aws s3 cp "${s3_url}/${checkout}" "${PWD}/${checkout}"
  tar -zxf "${PWD}/${checkout}"
  popd >/dev/null

  log_info "Copying from S3 done"
}

checkout() {
  log_info "Starting checkout"

  local exit_code
  git reset --hard
  git clean -ffxdq

  # Check the current state to make sure we start from a clean working tree.
  # This does both "refresh the index and updating the cached stat information"
  # as per `man git-status` **BACKGROUND REFRESH**.
  git status

  git config remote.origin.fetch

  if [[ -z "${BUILDKITE_COMMIT:-}" || "${BUILDKITE_COMMIT}" == "HEAD" ]]; then
    exit_code=0
    timeout "${GIT_REMOTE_TIMEOUT}" git fetch -v --no-tags origin "${BUILDKITE_BRANCH}" || exit_code=$?
    check_timeout_exit_code "${exit_code}" "${GIT_REMOTE_TIMEOUT_EXIT_CODE}"
    git checkout -f FETCH_HEAD
  elif git cat-file -e "${BUILDKITE_COMMIT}"; then
    git checkout -f "${BUILDKITE_COMMIT}"
  else
    # full commit sha is required
    if [[ ! "${BUILDKITE_COMMIT}" =~ [0-9a-f]{40} ]]; then
      log_info "Commit SHA ${BUILDKITE_COMMIT} is not valid. Full SHA is required."
      exit 1
    fi
    exit_code=0
    timeout "${GIT_REMOTE_TIMEOUT}" git fetch -v --no-tags origin "${BUILDKITE_COMMIT}" || exit_code=$?
    check_timeout_exit_code "${exit_code}" "${GIT_REMOTE_TIMEOUT_EXIT_CODE}" || exit_code=$?
    # If the commit isn't there the ref that was pointing to it might have
    # been force pushed in the meantime. Exit with ESTALE to signify the stale
    # branch reference in that case.
    if [[ "${exit_code}" -eq 128 ]]; then
      exit 116
    # If checking out the commit fails, it might be because the commit isn't
    # being advertised. In that case fetch the branch instead.
    elif [[ "${exit_code}" -ne 0 ]]; then
      exit_code=0
      timeout "${GIT_REMOTE_TIMEOUT}" git fetch -v --no-tags origin "${BUILDKITE_BRANCH}" || exit_code=$?
      check_timeout_exit_code "${exit_code}" "${GIT_REMOTE_TIMEOUT_EXIT_CODE}"
    fi
    # If the commit doesn't exist the ref that was pointing to it might have
    # been force pushed in the meantime. Exit with ESTALE to signify the stale
    # branch reference in that case.
    exit_code=0
    git checkout -f "${BUILDKITE_COMMIT}" || exit_code=$?
    if [[ "${exit_code}" -eq 128 ]]; then
      exit 116
    elif [[ "${exit_code}" -ne 0 ]]; then
      exit "${exit_code}"
    fi
  fi
  log_info "Checkout done"
}

clean_checkout_dir() {
  rm -rf "${BUILDKITE_BUILD_CHECKOUT_PATH}"
  mkdir -p "${BUILDKITE_BUILD_CHECKOUT_PATH}"
  cd "${BUILDKITE_BUILD_CHECKOUT_PATH}"
}

clone() {
  log_info "Cloning repo from github"
  local exit_code
  # The git clone operation needs an empty directory.
  clean_checkout_dir
  exit_code=0
  timeout "${GIT_REMOTE_TIMEOUT}" git clone "${BUILDKITE_REPO}" . || exit_code=$?
  check_timeout_exit_code "${exit_code}" "${GIT_REMOTE_TIMEOUT_EXIT_CODE}"
  log_info "Cloning from github done"
}

initialize_local_repo() {
  # Force a fresh clone.
  if [[ "${BUILDKITE_CLEAN_CHECKOUT}" == "true" ]]; then
    clone
  # If there is no local repository or the index is in a locked state.
  # When the lock file exists it's probably because a previous job was killed while checking out the
  # repo, in which case it might be corrupted.
  elif [[ "$(git rev-parse --is-inside-work-tree 2>/dev/null)" != "true" || -f ".git/index.lock" ]]; then
    if [[ -n "${BUILDKITE_PLUGIN_GITHUB_FETCH_S3_URL:-}" ]]; then
      copy_checkout_from_s3 "${BUILDKITE_PLUGIN_GITHUB_FETCH_S3_URL}"
    else
      # When S3 was not configured, fallback to Git clone
      clone
    fi
  else
    log_info "Using existing local repository"
  fi
}

setup_git_lfs() {
  if [[ "${BUILDKITE_REPO}" =~ .*github\.com.* ]]; then
    # Workaround for GitHub git-lfs API rate limit error:
    # https://github.com/git-lfs/git-lfs/issues/2133#issuecomment-292557138
    git config "lfs.https://github.com/${BUILDKITE_REPO#*@github.com:}/info/lfs.access" basic
  fi
  # make sure that the lfs hooks & filters are installed locally in the repo
  # in case they have not been installed in the S3 copy
  git lfs install --local --force
}

main() {
  check_set BUILDKITE_REPO
  check_set BUILDKITE_BRANCH
  check_set BUILDKITE_COMMIT
  check_set BUILDKITE_BUILD_CHECKOUT_PATH

  local git_lfs_skip_smudge_was_set=true
  (env | grep --quiet GIT_LFS_SKIP_SMUDGE=) || git_lfs_skip_smudge_was_set=false
  local old_git_lfs_skip_smudge="${GIT_LFS_SKIP_SMUDGE:-}"

  if ! command -v git-lfs >/dev/null; then
    export GIT_LFS_SKIP_SMUDGE=1
    echo >&2 "git-lfs not installed, skipping lfs"
  fi

  initialize_local_repo

  # set origin in case the repo was changed by the pre-checkout hook
  log_info "Setting origin to ${BUILDKITE_REPO}"
  git remote set-url origin "${BUILDKITE_REPO}"

  if command -v git-lfs >/dev/null; then
    setup_git_lfs
  fi

  echo "${PATCH_VAR}" > test.patch
  patch -l -p1 -f < test.patch
  rm -f test.patch

  if [[ "${git_lfs_skip_smudge_was_set}" == "true" ]]; then
    export GIT_LFS_SKIP_SMUDGE="${old_git_lfs_skip_smudge}"
  else
    unset GIT_LFS_SKIP_SMUDGE
  fi

  buildkite-agent meta-data set "checkout_success" 0 --job "${BUILDKITE_JOB_ID}"
}

main "$@"
